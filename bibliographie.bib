@article{
defossez2022a,
title={A Simple Convergence Proof of Adam and Adagrad},
author={Alexandre Defossez and Leon Bottou and Francis Bach and Nicolas Usunier},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ZPQhzTSWA7},
note={}
}
@article{
Kingma,
title={Adam : A method for stochastic optimization},
author={Diederik P. Kingma, Jimmy Lei Ba},
journal={ICLR 2015},
year={2015},
url={https://arxiv.org/pdf/1412.6980.pdf}
}

@article{
Adaptative,
title={Adaptative learning rate : AdaGrad and RMSprop},
author={Rauf Bhat},
journal={Towards data science},
year={2020},
url={https://towardsdatascience.com/adaptive-learning-rate-adagrad-and-rmsprop-46a7d547d244}
}

@article{
Weiss,
title={Elements d'analyse et d'optimisation convexe},
author={Pierre Weiss},
journal={Institut math√©matique de Toulouse},
year={2015},
url={https://www.math.univ-toulouse.fr/~weiss/Docs/LectureNotes_ConvexOptimization_PWeiss.pdf}
}
@article{
Yang,
title={Unified convergence analysis of stochastic momentum methods for
convex and non-convex optimization},
author={Tianbao Yang, Qihang Lin, and Zhe Li},
year={2016},
url={arXiv preprint arXiv:1604.03257}
}